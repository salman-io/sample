<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>SmileTracker — Web</title>
  <meta name="description" content="SmileTracker — detects smiles from your webcam and logs smile events." />
  <style>
    :root{--bg:#0f1724;--card:#0b1220;--muted:#9aa4b2;--accent:#22c55e}
    html,body{height:100%;margin:0;font-family:Inter,ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,'Helvetica Neue',Arial}
    body{background:linear-gradient(180deg,#071026 0%, #071827 100%);color:#e6eef8;display:flex;align-items:center;justify-content:center;padding:24px}
    .app{width:100%;max-width:980px;background:linear-gradient(180deg,rgba(255,255,255,0.02),transparent);border-radius:12px;padding:18px;box-shadow:0 8px 30px rgba(2,6,23,0.6)}
    header{display:flex;gap:12px;align-items:center}
    h1{font-size:20px;margin:0}
    p.lead{margin:0;color:var(--muted);font-size:13px}
    .grid{display:grid;grid-template-columns:1fr 360px;gap:12px;margin-top:12px}
    .card{background:var(--card);border-radius:10px;padding:12px}

    video{width:100%;height:auto;border-radius:8px;background:#000}
    canvas.overlay{position:absolute;left:0;top:0}

    .controls{display:flex;gap:8px;align-items:center;margin-top:8px}
    button{background:transparent;border:1px solid rgba(255,255,255,0.06);padding:8px 10px;border-radius:8px;color:inherit;cursor:pointer}
    button.primary{background:var(--accent);color:#042017;border:none}

    .status{font-size:13px;color:var(--muted);margin-top:8px}
    .log{font-family:monospace;font-size:13px;max-height:200px;overflow:auto;background:rgba(255,255,255,0.02);padding:8px;border-radius:6px}
    .small{font-size:12px;color:var(--muted)}

    footer{margin-top:12px;color:var(--muted);font-size:13px}

    @media (max-width:920px){.grid{grid-template-columns:1fr}}
  </style>
</head>
<body>
  <div class="app">
    <header>
      <div>
        <h1>SmileTracker — Web</h1>
        <p class="lead">Detect smiles with your webcam (runs in browser). Works on HTTPS / localhost.</p>
      </div>
    </header>

    <div class="grid">
      <div class="card">
        <div style="position:relative">
          <video id="video" autoplay muted playsinline></video>
          <canvas id="overlay" class="overlay"></canvas>
        </div>

        <div class="controls">
          <button id="startBtn" class="primary">Start Camera</button>
          <button id="stopBtn">Stop</button>
          <button id="snapshotBtn">Snapshot</button>
          <button id="downloadBtn">Download CSV</button>
        </div>

        <div class="status" id="status">Status: idle</div>
        <div style="display:flex;gap:12px;margin-top:8px">
          <div style="flex:1">
            <div class="small">Live smile score</div>
            <div id="score" style="font-size:20px;font-weight:600">—</div>
          </div>
          <div style="width:140px">
            <div class="small">Smiles detected</div>
            <div id="count" style="font-size:20px;font-weight:600">0</div>
          </div>
        </div>
      </div>

      <div class="card">
        <div class="small">Timeline (last 60s)</div>
        <canvas id="chart" height="180"></canvas>
        <div style="height:8px"></div>
        <div class="small">Event log</div>
        <div class="log" id="log"></div>
      </div>
    </div>

    <footer>
      <div>Note: Camera access requires HTTPS or localhost. To deploy: push this file to a GitHub repo and enable GitHub Pages (branch: main, folder: /).</div>
    </footer>
  </div>

  <!-- Libraries (CDNs) -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@4.10.0/dist/tf-core.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl@4.10.0/dist/tf-backend-webgl.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@1.2.2/dist/face-landmarks-detection.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>

  <script>
    // SmileTracker — single-file web app
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const ctx = overlay.getContext('2d');
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const snapshotBtn = document.getElementById('snapshotBtn');
    const downloadBtn = document.getElementById('downloadBtn');
    const statusEl = document.getElementById('status');
    const scoreEl = document.getElementById('score');
    const countEl = document.getElementById('count');
    const logEl = document.getElementById('log');

    let model = null;
    let stream = null;
    let rafId = null;
    let smiles = 0;
    let dataPoints = []; // last 60 seconds
    const SAMPLE_RATE_MS = 200;
    let lastSampleTime = 0;

    // Chart.js setup
    const chartCtx = document.getElementById('chart').getContext('2d');
    const chart = new Chart(chartCtx, {
      type: 'line',
      data: {
        labels: Array.from({length:60}, (_,i)=>''),
        datasets: [{label:'Smile score',data:Array.from({length:60},()=>0),tension:0.3}]
      },
      options:{animation:false,scales:{y:{min:0,max:1}},plugins:{legend:{display:false}}}
    });

    // Utility: distance
    function dist(a,b){return Math.hypot(a.x-b.x,a.y-b.y)}

    // Convert mediapipe landmarks (flat array) to points with x,y in pixels
    function lmToPoint(lm){return {x:lm[0]*video.videoWidth,y:lm[1]*video.videoHeight}}

    function log(msg){const t=new Date().toLocaleTimeString();logEl.innerHTML = `<div>[${t}] ${msg}</div>` + logEl.innerHTML}

    async function initModel(){
      statusEl.textContent = 'Status: loading model...';
      await tf.setBackend('webgl');
      model = await faceLandmarksDetection.load(faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);
      statusEl.textContent = 'Status: model loaded';
    }

    async function startCamera(){
      if (!model) await initModel();
      if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
        try{
          stream = await navigator.mediaDevices.getUserMedia({video:{facingMode:'user'},audio:false});
          video.srcObject = stream;
          await video.play();
          overlay.width = video.videoWidth;
          overlay.height = video.videoHeight;
          statusEl.textContent = 'Status: running';
          lastSampleTime = performance.now();
          rafId = requestAnimationFrame(loop);
          log('Camera started');
        }catch(e){statusEl.textContent='Status: camera error: '+e.message;log('Camera permission/error: '+e.message)}
      } else {
        statusEl.textContent = 'Status: getUserMedia not supported';
      }
    }

    function stopCamera(){
      if (rafId) cancelAnimationFrame(rafId);
      if (stream){
        stream.getTracks().forEach(t=>t.stop());
        stream = null;
      }
      video.pause();
      statusEl.textContent = 'Status: stopped';
      log('Camera stopped');
    }

    function snapshot(){
      const c = document.createElement('canvas');
      c.width = video.videoWidth; c.height = video.videoHeight;
      c.getContext('2d').drawImage(video,0,0);
      const a = document.createElement('a');
      a.href = c.toDataURL('image/png'); a.download = 'smile_snapshot.png'; a.click();
    }

    function downloadCSV(){
      let csv = 'timestamp,score,smile\n';
      for(const d of dataPoints){csv += `${new Date(d.t).toISOString()},${d.score.toFixed(3)},${d.smile ? 1 : 0}\n`} 
      const blob = new Blob([csv],{type:'text/csv'});
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a'); a.href = url; a.download = 'smile_log.csv'; a.click();
      URL.revokeObjectURL(url);
    }

    function pushData(score, smile){
      const now = Date.now();
      dataPoints.push({t:now,score,smile});
      // keep last 60 seconds
      const cutoff = now - 60_000;
      dataPoints = dataPoints.filter(d=>d.t >= cutoff);
      // update chart: 60 slots (one per second)
      const buckets = Array.from({length:60},()=>0);
      const counts = Array.from({length:60},()=>0);
      for(const d of dataPoints){
        const idx = Math.floor(((d.t - cutoff)/1000));
        const safe = Math.min(Math.max(idx,0),59);
        buckets[safe] += d.score; counts[safe]++;
      }
      const newData = buckets.map((s,i)=> counts[i]? (s/counts[i]) : 0);
      chart.data.datasets[0].data = newData.reverse().slice(0,60).reverse();
      chart.update();
    }

    async function loop(){
      rafId = requestAnimationFrame(loop);
      if (!model || video.readyState < 2) return;
      const now = performance.now();
      if (now - lastSampleTime < SAMPLE_RATE_MS) return;
      lastSampleTime = now;

      const preds = await model.estimateFaces({input:video});
      ctx.clearRect(0,0,overlay.width,overlay.height);
      if (preds && preds.length>0){
        // Use first face
        const face = preds[0];
        // landmarks is an array of [x,y,z] normalized coords (0..1) or pixel coords depending on package; here mediapipe gives normalized
        // Convert by multiplying by video dims
        const lm = face.scaledMesh || face.mesh || [];
        if (lm.length){
          // Mediapipe FaceMesh: mouth left=61, right=291, top=13, bottom=14
          const left = {x:lm[61][0], y:lm[61][1]};
          const right = {x:lm[291][0], y:lm[291][1]};
          const top = {x:lm[13][0], y:lm[13][1]};
          const bot = {x:lm[14][0], y:lm[14][1]};
          // If coordinates are normalized (0..1) detect by checking >1
          const norm = Math.max(left.x,left.y,right.x,right.y,top.x,top.y,bot.x,bot.y) <= 1;
          const toPx = p => ({x: p.x * video.videoWidth, y: p.y * video.videoHeight});
          const lpx = norm ? toPx(left) : {x:left.x, y:left.y};
          const rpx = norm ? toPx(right) : {x:right.x, y:right.y};
          const tpx = norm ? toPx(top) : {x:top.x, y:top.y};
          const bpx = norm ? toPx(bot) : {x:bot.x, y:bot.y};

          // Draw bounding landmarks
          ctx.strokeStyle = 'rgba(34,197,94,0.9)'; ctx.lineWidth = 2;
          ctx.beginPath(); ctx.arc(lpx.x,lpx.y,3,0,Math.PI*2); ctx.stroke();
          ctx.beginPath(); ctx.arc(rpx.x,rpx.y,3,0,Math.PI*2); ctx.stroke();

          // smile heuristic: ratio of mouth width to height
          const mouthW = dist(lpx,rpx);
          const mouthH = dist(tpx,bpx);
          const ratio = mouthW / Math.max(mouthH,1);
          // Map ratio to 0..1 (empirical): 1.2 -> 0, 2.8 -> 1
          const normScore = Math.min(Math.max((ratio - 1.2) / (2.8 - 1.2), 0), 1);

          scoreEl.textContent = normScore.toFixed(2);
          if (normScore > 0.6){
            // consider a smile
            if (!dataPoints.length || Date.now() - dataPoints[dataPoints.length-1].t > 1000){
              smiles += 1; countEl.textContent = smiles;
              log('Smile detected (score='+normScore.toFixed(2)+')');
            }
          }
          pushData(normScore, normScore>0.6);
        }
      } else {
        scoreEl.textContent = '0.00';
        pushData(0,false);
      }
    }

    startBtn.addEventListener('click',()=>startCamera());
    stopBtn.addEventListener('click',()=>stopCamera());
    snapshotBtn.addEventListener('click',()=>snapshot());
    downloadBtn.addEventListener('click',()=>downloadCSV());

    // Auto-start if HTTPS and allowed
    if (location.protocol==='https:' || location.hostname==='localhost'){
      // optionally auto-start
      // startCamera();
    }
  </script>
</body>
</html>
